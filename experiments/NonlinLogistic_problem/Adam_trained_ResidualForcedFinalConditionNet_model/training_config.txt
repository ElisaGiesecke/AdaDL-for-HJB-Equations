------------------------------------
 Adam Training
------------------------------------
optimizer:                      Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
batch size:                     100
gradient regularization weight: 100.0
maximum epochs:                 200
maximum time:                   1000.0
early stopping:                 error metric = [0.0, 0.0, 1.0], strip = 5, number of strips = 1, maximum iterations = 50, minimum progress = 0.01, criterion = PQ, tolerance = 0.1
dataset size selection:         convergence tolerance = 0.0001, growth upper bound = 1.25
